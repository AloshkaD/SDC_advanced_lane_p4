{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read in an image using matplotlib.image.imread() you will get an RGB image, but if you read it in using OpenCV cv2.imread() this will give you a BGR image. \n",
    "\n",
    "hls = cv2.cvtColor(im, cv2.COLOR_RGB2HLS) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import glob\n",
    "import math\n",
    "from skimage.feature import corner_harris,corner_peaks\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib qt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaussian_noise(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255   \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def transform(img):\n",
    "    imshape = img.shape\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    src=np.float32([[160,imshape[0]],[imshape[1]/2-60, imshape[0]/2+90],[imshape[1]/2+100, imshape[0]/2+90], [imshape[1]-20,imshape[0]]])\n",
    "    dst=np.float32([[(240,imshape[0]),(240, 0),(imshape[1]-130, 0), (imshape[1]-130,imshape[0])]])\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    wraped =  cv2.warpPerspective(img,M,img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return  Minv, wraped\n",
    "\n",
    "# Read in an image and grayscale it\n",
    " \n",
    "## return sobel threshold\n",
    "def abs_sobel_thresh(img, orient, sobel_kernel, thresh):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    #binary_output[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "## return mag_direction\n",
    "\n",
    "def mag_thresh(img, sobel_kernel, mag_thresh):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output\n",
    "\n",
    "## return the gradient\n",
    "\n",
    "def dir_threshold(img, sobel_kernel, thresh=(0, np.pi/2)):\n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx=np.absolute(sobelx)\n",
    "    abs_sobely=np.absolute(sobely)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    dir_grad = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    binary_output = np.zeros_like(dir_grad)\n",
    "    binary_output[(dir_grad >= thresh[0]) & (dir_grad <= thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    #binary_output = np.copy(img) # Remove this line\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "\n",
    "class Left:\n",
    "    def __init__(self):\n",
    "        # Was the line found in the previous frame?\n",
    "        self.found = False\n",
    "        \n",
    "        # Remember x and y values of lanes in previous frame\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        # Store recent x intercepts for averaging across frames\n",
    "        self.x_int = []\n",
    "        self.top = []\n",
    "        \n",
    "        # Remember previous x intercept to compare against current one\n",
    "        self.lastx_int = None\n",
    "        self.last_top = None\n",
    "        \n",
    "        # Remember radius of curvature\n",
    "        self.radius = None\n",
    "        \n",
    "        # Store recent polynomial coefficients for averaging across frames\n",
    "        self.fit0 = []\n",
    "        self.fit1 = []\n",
    "        self.fit2 = []\n",
    "        self.fit3 = []\n",
    "        self.fit4 = []\n",
    "        self.fit5 = []\n",
    "        self.fitx = None\n",
    "        self.pts = []\n",
    "        \n",
    "        # Count the number of frames\n",
    "        self.count = 0\n",
    "\n",
    "        \n",
    "class Right:\n",
    "    def __init__(self):\n",
    "        # Was the line found in the previous frame?\n",
    "        self.found = False\n",
    "        \n",
    "        # Remember x and y values of lanes in previous frame\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        # Store recent x intercepts for averaging across frames\n",
    "        self.x_int = []\n",
    "        self.top = []\n",
    "        \n",
    "        # Remember previous x intercept to compare against current one\n",
    "        self.lastx_int = None\n",
    "        self.last_top = None\n",
    "        \n",
    "        # Remember radius of curvature\n",
    "        self.radius = None\n",
    "        \n",
    "        # Store recent polynomial coefficients for averaging across frames\n",
    "        self.fit0 = []\n",
    "        self.fit1 = []\n",
    "        self.fit2 = []\n",
    "        self.fit3 = []\n",
    "        self.fit4 = []\n",
    "        self.fit5 = []\n",
    "        self.fitx = None\n",
    "        self.pts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform camera calibration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        #write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "        #cv2.imwrite(write_name, img)\n",
    "        \n",
    "        #cv2.imshow('img', img)\n",
    "        #cv2.waitKey(500)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Implement calibration on the images that will be used\n",
    "\n",
    "\n",
    "#dst = cv2.cvtColor(dst, cv2.COLOR_RGB2BGR)\n",
    "    #cv2.imwrite('test_images/test6.jpg',dst)\n",
    "\n",
    "# Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "#dist_pickle = {}\n",
    "#dist_pickle[\"mtx\"] = mtx\n",
    "#dist_pickle[\"dist\"] = dist\n",
    "#pickle.dump( dist_pickle, open( \"calibration_wide/wide_dist_pickle.p\", \"wb\" ) )\n",
    "\n",
    "#dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Visualize undistortion\n",
    "#f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "#ax1.imshow(img)\n",
    "#ax1.set_title('Original Image', fontsize=30)\n",
    "#ax2.imshow(dst)\n",
    "#ax2.set_title('Undistorted Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_vid(image):\n",
    " \n",
    "\n",
    "    # Test undistortion on an image\n",
    "    #img = cv2.imread('test_images/Distorted/test6.jpg')\n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "#img = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "# Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "\n",
    "\n",
    "    undist = cv2.undistort(image, mtx, dist, None, mtx)  \n",
    "#Read in the image\n",
    "#image = mpimg.imread('test_images/test6.jpg')\n",
    "#Blur the image\n",
    "    blur_kernel_size = 1\n",
    "    image = gaussian_noise(image, blur_kernel_size)\n",
    "\n",
    "#Define a mask but only implement it after edge detection dot not be detected\n",
    "    imshape = image.shape\n",
    "        #vertices = np.array([[(80,imshape[0]),(400, 330), (580, 330), (imshape[1],imshape[0])]], dtype=np.int32)\n",
    "    vertices = np.array([[(160,imshape[0]),(imshape[1]/2-60, imshape[0]/2+90),\n",
    "                      (imshape[1]/2+100, imshape[0]/2+90), (imshape[1]-20,imshape[0])]], dtype=np.int32)\n",
    "    vertices = np.array([[(160,imshape[0]),(imshape[1]/2-60, imshape[0]/2+90),\n",
    "                      (imshape[1]/2+100, imshape[0]/2+90), (imshape[1]-20,imshape[0])]], dtype=np.int32)\n",
    "#image = region_of_interest(image, vertices)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    thresh = (220, 255)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "    binary = np.zeros_like(gray)\n",
    "    binary[(gray > thresh[0]) & (gray <= thresh[1])] = 1\n",
    "#f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "#f.tight_layout()\n",
    "#ax1.imshow(image)\n",
    "#ax1.set_title('Original Image', fontsize=50)\n",
    "#ax2.imshow(binary, cmap='gray')\n",
    "#ax2.set_title('Thresholded Gradient', fontsize=50)\n",
    "#plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "    \n",
    "\n",
    "# Splitting RGB Channels\n",
    "    R = image[:,:,0]\n",
    "    G = image[:,:,1]\n",
    "    B = image[:,:,2]\n",
    "    thresh = (220, 255)\n",
    "#gray_ = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    binary_R = np.zeros_like(R)\n",
    "    binary_R[(R > thresh[0]) & (R <= thresh[1])] = 1\n",
    "    binary_R= region_of_interest(binary_R, vertices)\n",
    "\n",
    "    binary_G = np.zeros_like(G)\n",
    "    binary_G[(G > thresh[0]) & (G <= thresh[1])] = 1\n",
    "    binary_G= region_of_interest(binary_G, vertices)\n",
    "\n",
    "    binary_B = np.zeros_like(B)\n",
    "    binary_B[(B > thresh[0]) & (B <= thresh[1])] = 1\n",
    "    binary_B= region_of_interest(binary_B, vertices)\n",
    "    \n",
    "    # Splitting HSV Channels\n",
    "    HLS = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "\n",
    "    H = HLS[:,:,0]\n",
    "    L = HLS[:,:,1]\n",
    "    S = HLS[:,:,2]\n",
    "    thresh = (150, 255)\n",
    "#gray_ = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    binary_H = np.zeros_like(H)\n",
    "    binary_H[(H > thresh[0]) & (H <= thresh[1])] = 1\n",
    "    binary_H= region_of_interest(binary_H, vertices)\n",
    "\n",
    "    binary_L = np.zeros_like(L)\n",
    "    binary_L[(L > thresh[0]) & (L <= thresh[1])] = 1\n",
    "    binary_L= region_of_interest(binary_L, vertices)\n",
    "\n",
    "    binary_S = np.zeros_like(S)\n",
    "    binary_S[(S > thresh[0]) & (S <= thresh[1])] = 1\n",
    "    binary_S= region_of_interest(binary_S, vertices)\n",
    "    ksize=3\n",
    "\n",
    "    gradx = abs_sobel_thresh(image, orient='x', sobel_kernel=ksize, thresh=(80, 100))\n",
    "    gradx = region_of_interest(gradx, vertices)\n",
    "    grady = abs_sobel_thresh(image, orient='y', sobel_kernel=ksize, thresh=(80, 100))\n",
    "    grady = region_of_interest(grady, vertices)\n",
    "    mag_binary = mag_thresh(image, sobel_kernel=ksize, mag_thresh=(30, 100))\n",
    "    mag_binary = region_of_interest(mag_binary, vertices)\n",
    "    dir_binary = dir_threshold(image, sobel_kernel=ksize, thresh=(0.3, 1.5)) \n",
    "    dir_binary = region_of_interest(dir_binary, vertices)\n",
    "\n",
    "#dir_binary = dir_threshold(image, sobel_kernel=15, thresh=(0.7, 1.3))\n",
    "\n",
    "    combined = np.zeros_like(dir_binary)\n",
    "    combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "\n",
    "\n",
    "#Combine all combined binary and R channel\n",
    "\n",
    "    combined_binary_R = np.zeros_like(combined )\n",
    "    combined_binary_R[(binary_R== 1) | (combined == 1)] = 1\n",
    "\n",
    "#color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "#Combine all combined binary and S channel\n",
    "\n",
    "    combined_binary_S = np.zeros_like(combined )\n",
    "    combined_binary_S[(binary_S== 1) | (combined == 1)] = 1\n",
    "\n",
    "#Combine Sobel and R channel\n",
    "\n",
    "    Sobel_binary_R = np.zeros_like(gradx)\n",
    "    Sobel_binary_R[(binary_R== 1) | (gradx== 1)] = 1\n",
    "\n",
    "#Combine all combined binary and S channel\n",
    "\n",
    "    Sobel_binary_S = np.zeros_like(gradx )\n",
    "    Sobel_binary_S[(binary_S== 1) | (gradx == 1)] = 1\n",
    "    \n",
    "    Minv, warped_img= transform(Sobel_binary_S)\n",
    "    row_w,col_w=warped_img.shape\n",
    "\n",
    "    warped_img_left=warped_img[0:row_w,0:math.ceil(col_w/2)]\n",
    "    warped_img_right=warped_img[0:row_w,math.ceil(col_w/2):col_w]\n",
    "#def show_corners(corners_l, corners_r,image,title=None):\n",
    "    \"\"\"Display a list of corners overlapping an image\"\"\"\n",
    "    #fig = plt.figure()\n",
    "    #plt.imshow(image,cmap='gray')\n",
    "    y_corner_l = []\n",
    "    x_corner_l = []\n",
    "    y_corner_r = []\n",
    "    x_corner_r = []\n",
    " \n",
    "    corners_left = corner_peaks(corner_harris(warped_img_left),min_distance=2)\n",
    "    corners_right = corner_peaks(corner_harris(warped_img_right),min_distance=2)\n",
    "    y_corner_l,x_corner_l = zip(*corners_left)\n",
    "\n",
    "    adjusted_corners_right= corners_right+[0,math.ceil(col_w/2)]\n",
    "                                      \n",
    "    y_corner_r,x_corner_r = zip(*adjusted_corners_right)\n",
    "    #Measuring Curvature\n",
    "    yvalus = warped_img.shape[0]\n",
    " # Generate some fake data to represent lane-line pixels\n",
    "    yvals = np.linspace(0, 100, num=101)*(yvalus/100.0)  # to cover same y-range as image\n",
    "\n",
    "#leftx = np.array([200 + (elem**2)*4e-4 + np.random.randint(-50, high=51) \n",
    "                              #for idx, elem in enumerate(yvals)])\n",
    "#leftx = leftx[::-1]  # Reverse to match top-to-bottom in y\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    leftx = np.array(x_corner_l)\n",
    "    lefty = np.array(y_corner_l)\n",
    "\n",
    "#rightx = np.array([900 + (elem**2)*4e-4 + np.random.randint(-50, high=51) \n",
    "                                #for idx, elem in enumerate(yvals)])\n",
    "#rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    rightx = np.array(x_corner_r)\n",
    "    righty = np.array(y_corner_r)\n",
    "# Fit a second order polynomial to each fake lane line\n",
    "\n",
    "    \n",
    "    \n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    \n",
    "    # Calculate intercepts to extend the polynomial to the top and bottom of warped image\n",
    "    leftx_int = left_fit[0]*720**2 + left_fit[1]*720 + left_fit[2]\n",
    "    left_top = left_fit[0]*0**2 + left_fit[1]*0 + left_fit[2]\n",
    "    \n",
    "    # Average intercepts across 5 frames\n",
    "    Left.x_int.append(leftx_int)\n",
    "    Left.top.append(left_top)\n",
    "    leftx_int = np.mean(Left.x_int)\n",
    "    left_top = np.mean(Left.top)\n",
    "    Left.lastx_int = leftx_int\n",
    "    Left.last_top = left_top\n",
    "    leftx = np.append(leftx, leftx_int)\n",
    "    lefty = np.append(lefty, 720)\n",
    "    leftx = np.append(leftx, left_top)\n",
    "    lefty = np.append(lefty, 0)\n",
    "    lsort = np.argsort(lefty)\n",
    "    lefty = lefty[lsort]\n",
    "    leftx = leftx[lsort]\n",
    "    Left.X = leftx\n",
    "    Left.Y = lefty\n",
    "    \n",
    "    # Recalculate polynomial with intercepts and average across 5 frames\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    Left.fit0.append(left_fit[0])\n",
    "    Left.fit1.append(left_fit[1])\n",
    "    Left.fit2.append(left_fit[2])\n",
    " \n",
    "    left_fit = [np.mean(Left.fit0), np.mean(Left.fit1), np.mean(Left.fit2)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]\n",
    "    Left.fitx = left_fitx\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]\n",
    "\n",
    "#extrapolate to fit the line in top and bottom\n",
    "#right_fit = np.polyfit(righty, rightx, 2)\n",
    "#right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]\n",
    "# Calculate intercepts to extend the polynomial to the top and bottom of warped image\n",
    "\n",
    "    rightx_int = right_fit[0]*720**2 + right_fit[1]*720 + right_fit[2]\n",
    "    right_top = right_fit[0]*0**2 + right_fit[1]*0 + right_fit[2]\n",
    "# Average intercepts across 5 frames\n",
    "    Right.x_int.append(rightx_int)\n",
    "    rightx_int = np.mean(Right.x_int)\n",
    "    Right.x_int.append(rightx_int)\n",
    "    rightx_int = np.mean(Right.x_int)\n",
    "    Right.lastx_int = rightx_int\n",
    "    Right.last_top = right_top\n",
    "    rightx = np.append(rightx, rightx_int)\n",
    "    righty = np.append(righty, 720)\n",
    "    rightx = np.append(rightx, right_top)\n",
    "    righty = np.append(righty, 0)\n",
    "    rsort = np.argsort(righty)\n",
    "    righty = righty[rsort]\n",
    "    rightx = rightx[rsort]\n",
    "    Right.X = rightx\n",
    "    Right.Y = righty\n",
    "# Recalculate polynomial with intercepts and average across 5 frames\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    Right.fit0.append(right_fit[0])\n",
    "    Right.fit1.append(right_fit[1])\n",
    "    Right.fit2.append(right_fit[2])\n",
    " \n",
    "    right_fit = [np.mean(Right.fit0), np.mean(Right.fit1), np.mean(Right.fit2)]\n",
    "# Fit polynomial to detected pixels\n",
    "    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]\n",
    "    Right.fitx = right_fitx     \n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    left_fitx = left_fit[0]*lefty**2 + left_fit[1]*lefty + left_fit[2]\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    right_fitx = right_fit[0]*righty**2 + right_fit[1]*righty + right_fit[2]\n",
    " \n",
    "    y_eval_l = np.max(lefty)\n",
    "    y_eval_r = np.max(lefty)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*y_eval_l + left_fit[1])**2)**1.5) \\\n",
    "                                 /np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*y_eval_r + right_fit[1])**2)**1.5) \\\n",
    "                                /np.absolute(2*right_fit[0])\n",
    "    #print(left_curverad, right_curverad)\n",
    "# Example values: 1163.9    1213.7\n",
    "              # Calculate the position of the vehicle\n",
    "    center = abs(640 - ((rightx_int+leftx_int)/2)*3.7/700)\n",
    "# Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meteres per pixel in x dimension\n",
    "\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval_l + left_fit_cr[1])**2)**1.5) \\\n",
    "                                 /np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval_r + right_fit_cr[1])**2)**1.5) \\\n",
    "                                /np.absolute(2*right_fit_cr[0])\n",
    "# Now our radius of curvature is in meters\n",
    "    #print(left_curverad, 'm', right_curverad, 'm')\n",
    "# Example values: 3380.7 m    3189.3 m\n",
    "\n",
    " \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "#src = np.float32([[490, 482],[810, 482],[1250, 720],[40, 720]])\n",
    "#dst = np.float32([[0, 0], [1280, 0],[1250, 720],[40, 720]])\n",
    "#Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "\n",
    "# Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped_img).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "# Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, lefty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, righty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    \n",
    "# Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "     \n",
    "# Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "# Combine the result with the original image\n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Remember recent polynomial coefficients and intercepts\n",
    "    if len(Left.fit0) > 10:\n",
    "        Left.fit0 = Left.fit0[1:]\n",
    "    if len(Left.fit1) > 10:\n",
    "        Left.fit1 = Left.fit1[1:]\n",
    "    if len(Left.fit2) > 10:\n",
    "        Left.fit2 = Left.fit2[1:]\n",
    "    if len(Left.x_int) > 50:\n",
    "        Left.x_int = Left.x_int[1:]\n",
    "    if len(Left.top) > 50:\n",
    "        Left.top = Left.top[1:]\n",
    "    if len(Right.fit0) > 10:\n",
    "        Right.fit0 = Right.fit0[1:]\n",
    "    if len(Right.fit1) > 10:\n",
    "        Right.fit1 = Right.fit1[1:]\n",
    "    if len(Right.fit2) > 10:\n",
    "        Right.fit2 = Right.fit2[1:]\n",
    "    if len(Right.x_int) > 50:\n",
    "        Right.x_int = Right.x_int[1:]\n",
    "    if len(Right.top) > 50:\n",
    "        Right.top = Right.top[1:]\n",
    "\n",
    "\n",
    "    final_result=cv2.putText(result, \"Left:{}\".format(left_curverad),(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)\n",
    "    final_result=cv2.putText(result, \"Right:{}\".format(right_curverad),(10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)\n",
    "    if center < 640:\n",
    "            final_result=cv2.putText(result, \"Vehicle is {:.2f}m left of center\".format(center*3.7/700),(10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "    else:\n",
    "            final_result=cv2.putText(result, \"Vehicle is {:.2f}m right of center\".format(center*3.7/700),(10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "\n",
    "    #plt.imshow(final_result)\n",
    "\n",
    "    ###############    \n",
    "    #f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "     \n",
    "    #ax1.imshow(final_result)\n",
    "    #ax1.set_title('Binary Image', fontsize=50)\n",
    "    #ax2.imshow(final_result)\n",
    "    #ax2.set_title('Warped Binary', fontsize=50)\n",
    "###############\n",
    " \n",
    "    Left.count += 1\n",
    "    return result\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective transform\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f7f8921df04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mRight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvideo_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_video_output.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project_video.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mwhite_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_vid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwhite_clip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ros/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, audio_fps, audio_nbytes, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mpix_fmt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFFMPEG_VideoReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpix_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpix_fmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ros/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_parse_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ros/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mpopen_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"creationflags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0x08000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ros/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ros/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1491\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "Left.__init__(Left)\n",
    "Right.__init__(Right)\n",
    "video_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(process_vid) \n",
    "white_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"360\" controls>\n",
       "  <source src=\"project_video_output.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format('project_video_output.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-TensorFlow-Lab]",
   "language": "python",
   "name": "conda-env-CarND-TensorFlow-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
